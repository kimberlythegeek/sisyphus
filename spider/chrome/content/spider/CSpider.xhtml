<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <title>
      CSpider - a JavaScript Class for Spidering Web pages
    </title>
  </head>
  <body>
    <h1>CSpider</h1>

    <h2>NAME</h2>

    <p>
      CSpider - a JavaScript Class for Spidering Web pages
    </p>

    <h2>SYNOPSIS</h2>

<pre><![CDATA[<script type="text/javascript" src="CCallWrapper.js"></script>
<script type="text/javascript" src="CSpider.js"></script>
<script type="text/javascript">

function CPageLoader()
{
  // customize per loader
}

CPageLoader.load = 
function CPageLoader_loadPage(/* String */ url, /* String */ referer)
{
  // customize to initiate loading a new page
};

CPageLoader.cancel =
function CPageLoader_cancel()
{
  // customize to cancel loading a page
};

CPageLoader.getDocument =
function CPageLoader_getDocument()
{
  // customize to obtain a reference to 
  // the loaded document.
};

var aUrl = 'http://example.com';
var aRestrictUrl = true;
var aDepth = 1;
var aPageLoader = new CPageLoader();
var aOnLoadTimeoutInterval = 120;

var spider = new CSpider(aUrl, 
                         aDomain, 
                         aRestrictUrl, 
                         aDepth, 
                         aPageLoader, 
                         aOnLoadTimeoutInterval,
                         aExtraPrivileges,
                         aRespectRobotRules,
                         aUserAgent);

spider.mOnStart       = function() { /* customize */ };
spider.mOnBeforePage  = function() { /* customize */ };
spider.mOnAfterPage   = function() { /* customize */ };
spider.mOnPageTimeout = function() { /* customize */ };
spider.mOnStop        = function() { /* customize */ };
spider.mOnPause       = function() { /* customize */ };
spider.mOnRestart     = function() { /* customize */ };

spider.run();

</script>
]]></pre>

    <h2>DESCRIPTION</h2>

    <p>

    <code>CSpider</code> is a JavaScript class which is intended to be used
    as a framework upon which web spidering applications can be built. Through
    the use of the user overridable methods, <code>CSpider</code>'s behavior
    can be customized to meet the needs of many types of applications.

    </p>

    <p>

    <code>CSpider</code> uses a helper class to abstract and manage requests to
    load external documents which must implement the following methods:

    </p>

    <ul>
      <li>load(String url)</li>
      <li>cancel()</li>
      <li>Document getDocument()</li>
    </ul>

    <h3>Constructor</h3>
<pre><![CDATA[function CSpider(/* String */ aUrl, 
                 /* String */ aDomain,
                 /* Boolean */ aRestrictUrl,
                 /* Number */ aDepth, 
                 /* CPageLoader */ aPageLoader,
                 /* Seconds */ aOnLoadTimeoutInterval,
                 /* Boolean */ aExtraPrivileges,
                 /* Boolean */ aRespectRobotRules,
                 /* String */ aUserAgent)]]></pre>

    <p>
    Constructs an instance of <code>CSpider</code> which will begin
    spidering at the location <code>aUrl</code>, optionally restricting
    visited pages to include the string <code>aDomain</code> (if 
    <code>aRestrictUrl</code> is true) up to a depth of <code>aDepth</code>
    while waiting for <code>aOnLoadTimeoutInterval</code> seconds for
    each page to load before timing out. <code>aPageLoader</code> is
    a special helper object used to manager loading external pages.
    <code>aExtraPrivilges</code> can be set to true in Gecko in order
    to use <code>netscape.security.PrivilegeManager</code> to request
    additional security privileges in order to obtain cross-domain
    access to loaded page's DOM.
    </p>

    <p>
    If aRespectRobotRules is true, CSpider will load robots.txt from
    each domain it visits using isRobotBlocked.js and will not add
    pages to its pending list if they are blocked by the robot rules.
    </p>

    <p>
    aUserAgent is currently passed to isRobotBlocked() however is not
    (yet) used to set the user agent of the client. If not passed as
    an argument, aUserAgent is assumed to be 'Gecko/'.

    </p>

    <h3>Properties</h3>

    <h4>mUrl</h4>

    <p>

    String containing the initial URL where CSpider will begin. If the 
    initial value obtained from aUrl in the constructor does not begin
    with 'http://' or 'https://' or 'file://', 'http://' will be prepended to aUrl before
    it is saved in mUrl.

    </p>

    <h4>mDomain</h4>

    <p>

    String which will be used (if mRestrictUrl is true) to filter links
    in loaded documents. If aDomain is not specified, aDomain will be 
    set to the value of aUrl with any leading protocol ('http://' or 'https://')
    removed.

    </p>

    <h4>mRestrictUrl</h4>

    <p>

    Boolean which if true will cause CSpider to only follow links which
    contain the String mDomain.

    </p>

    <h4>mDepth</h4>

    <p>

    Number indicating the depth (number of clicks) to which CSpider will
    follow. 

    </p>

    <h4>mCallWrapperOnLoadPageTimeout</h4>

    <p>

    An instance of CCallWrapper which is used to detect timeout 
    conditions when loading pages.

    </p>

    <h4>mOnLoadTimeoutInterval</h4>

    <p>

    Number (in milliseconds) that CSpider will wait for a page to load
    before calling mCallWrapperOnLoadpageTimeout.

    </p>

    <h4>mExtraPrivileges</h4>

    <p>

    Boolean which invokes
    <code>netscape.security.PrivilegeManager.enablePrivilege</code> to enable
    cross-domain spidering.

    </p>

    <h4>mRespectRobotRules</h4>

    <p>

    Boolean which determines if CSpider will load and check urls of 
    pages to be loaded against a site's robots.txt file using 
    isRobotBlocked().

    </p>

    <h4>mUserAgent</h4>

    <p>

    String which is passed to isRobotBlocked() to check the robot rules
    however currently defaults to 'Gecko/'.

    </p>

    <h4>mPagesVisited</h4>

    <p>

    An Array of Strings of the URLS visited by CSpider.

    </p>

    <h4>mPagesPending</h4>

    <p>

    An Array of CUrl instances representing the pages to be 
    visited.

    </p>

    <h4>mPagesHash</h4>

    <p>

    A hash of URL strings which are used to determine pages which have
    already been visited and which do not need to be revisited.

    </p>

    <h4>mState</h4>

    <p>

    A String indicating the state of the spider. One of 'ready',
    'running', 'pausing', 'paused', 'timeout', 'stopping', 'stopped'.

    </p>

    <h4>mCurrentUrl</h4>

    <p>

    An instance of CUrl representing the page currently being loaded.
    CUrl is an internal class used to keep track of information related to 
    the currently loaded url.

    </p>

<div style="margin-left: 5em; border: solid 1px black;">
<h5>Constructor</h5>
<pre><![CDATA[
function CUrl(/* Number */ aDepth,
              /* String */ aUrl,
              /* String */ aReferer);
]]></pre>

<h5>Properties</h5>
    <h6>mDepth</h6>
    <p>the depth
    of the URL</p>

    <h6>mUrl</h6>
    <p>a String containing the actual URL</p>
<h6>mReferer</h6>
<p>referer for 
    the request to load this url</p>
    <h6>mResponses</h6>
    <p>an array of objects containing
    captured HTTP response data from loading the url. Note that
    mResponses[0] contains the responses for the page mUrl while
    mResponses[i] for i &gt; 0 are the responses for the scripts,
    CSS files, images, etc loaded by the document at mUrl.

    </p>
    <p>
    Each object in mResponses[] contains data from the 
    <a href="http://lxr.mozilla.org/mozilla/source/netwerk/protocol/http/public/nsIHttpChannel.idl">nsIHttpChannel</a> 
    interface in the following properties:
    </p>
    <dl>
    <dt>originalURI</dt>
    <dd><p>The original uri requested</p></dd>
    <dt>URI</dt>
    <dd><p>The actual uri returned</p></dd>
    <dt>referrer</dt>
    <dd><p>The referrer for the URI</p></dd>
    <dt>responseStatus</dt>
    <dd><p>The HTTP status code for the response</p></dd>
    <dt>responseStatusText</dt>
    <dd><p>Lower case status in text form, e.g. ok, moved, etc.</p></dd>
    <dt>contentType</dt>
    <dd><p>The content type of the document returned</p></dd>
    <dt>requestSucceeded</dt>
    <dd><p>Boolean signifying if the request was successful.</p></dd>

    </dl>

</div>
    <h4>mDocument</h4>

    <p>

    A reference to the currently loaded Document. This property is set
    to null in loadPage() and set to the currently loaded document by
    onLoadPage().

    </p>

    <h3>Controller Methods</h3>

    <p>

    These methods are used to control the spider.

    </p>

    <h4>run</h4>

    <p>

    run() begins spidering at mUrl. run() enters the 'running' state, then
    calls the user specified mOnStart() before loading any pages. mOnStart
    controls the execution of the spider by returning true to begin loading
    pages or false to prevent loading pages and re-enter the 'ready' state.

    </p>

    <h4>restart</h4>

    <p>

    If the spider is in either the 'paused' or 'timeout' states, 
    restart() will enter the 'running' state, then call the user specified
    mOnRestart() before loading any pages. mOnRestart controls the
    execution of the spider by returning true to begin loading pages or
    false to prevent loading pages and enter the 'paused' state.

    </p>

    <h4>pause</h4>

    <p>

    If mCallWrapperOnLoadPageTimeout is not null, pause() enters the 'pausing'
    state and then returns to allow either onLoadPage or onLoadPageTimeout to
    complete before entering the 'paused' state. If
    mCallWrapperOnLoadPageTimeout is null, pause() enters the 'paused' state
    then calls mOnPause() to control the execution of the spider by returning
    true to stay in the paused state or false to enter the 'running' state and
    call loadPage() to continue spidering.

    </p>

    <h4>stop</h4>

    <p>

    If mCallWrapperOnLoadPage is not null, stop() enters the 'stopping' sate
    and then returns to allow either onLoadPage or onLoadPageTimeout to
    complete before entering the 'stopped' state. If
    mCallWrapperOnLoadPageTimeout is null, stop() enters the 'stopped' state,
    then calls mOnStop() to control the execution of the spider by returning
    true to stay in the stopped state or false to enter the 'running' state and
    call loadPage() to continue spidering.

    </p>

    <h3>Internal Methods</h3>

    <p>

    These methods are used internally by CSpider.

    </p>

    <h4>init</h4>

    <p>

    init() is a private method used to initialize the CSpider.

    </p>

    <h4>addPage</h4>

    <p>

    addPage(aUrl) is a private method called by onLoadPage() to add the URL
    aUrl to the list of pending URLs to be visited. addPage limits the pages to
    be visited by rejecting any URLs which 
    
    </p>
    
    <ul>

      <li>do not have either 'http' or 'https' protocols</li>
      <li>have already been added</li>
      <li>exceed the specified depth</li>
      <li>if mRestrictUrl is true, rejecting any URLs which do not
      contain mDomain</li>
      <li>end in one of a set of 'bad extensions'. These extensions are 
      currently hard-coded into addPage</li>
    
    </ul>

    <h4>loadPage</h4>

    <p>

    loadPage() initiates the process of loading the next page from the list
    maintained in mPagesPending. If the spider is not in the 'running' state,
    loadPage does nothing.

    </p>

    <p>

    loadPage removes the next CUrl object from the mPagesPending stack and
    calls stop() if there are no more pages pending. mOnBeforePage() is called
    to control execution of the spider  by returning true to begin the load or
    false to prevent the load and to enter the 'paused' state. If mOnBeforePage
    prevents the load, the CUrl object is placed back on the mPagesPending
    stack.

    </p>

    <p>

    An instance of a CCallWrapper for onLoadPageTimeout is created, saved to
    mCallWrapperOnLoadPageTimeout and asynchronously executed to detect page
    timeouts and the mPageLoader's load() method is called to initiate the
    loading of the next page.

    </p>

    <h4>onLoadPageTimeout</h4>

    <p>

    If the mCallWrapperOnLoadPageTimeout has not been cancelled by onLoadPage,
    onLoadPageTimeout will execute.
    
    </p>
    
    <p>
    
    If the spider is in the 'pausing' state,
    pause() is called and the spider enters the 'paused' state. If the spider
    is in the 'stopping' state, stop() is called and the spider enters the
    'stopped' state.
    
    </p>
    
    <p>
    
    The user defined mOnPageTimeout() is called to control
    the execution of the spider. If mOnPageTimeout returns true the spider will
    enter the 'timeout' state, otherwise the spider enters the 'running' state
    and loadPage will be called to continue spidering the site.

    </p>

    <h4>onLoadPage</h4>

    <p>

    onLoadPage() is called by the implementation of the page loader 
    when each page has completed loading. onLoadPage will cancel 
    mCallWrapperOnLoadPageTimeout
    
    </p>
    

    <p>

    If the spider is in the 'pausing' state, pause() is called and the spider
    enters the 'paused' state. If the spider is in the 'stopping' state, stop()
    is called and the spider enters the 'stopped' state. 
    
    </p>
    
    <p>
    
    onLoadPage adds the current mCurrentUrl to the
    mPagesVisited array, saves a reference to the loaded document in
    mDocument and call addPage for each link, frame and iframe in the 
    document.

    </p>
    <p>
    
    onLoadPage calls the
    user specified mOnAfterPage to control the execution of the spider.  If
    mOnAfterPage returns true, onLoadPage will call loadPage to begin loading
    the next page. if mOnAfterPage returns false, the spider will enter the
    'paused' state.

    </p>

    <h4>cancelLoadPage</h4>

    <p>

    cancelLoadPage() calls mCallWrapperOnLoagePageTimeout.cancel()
    to cancel any pending page load timeouts and mPageLoader.cancel()
    to cancel any pages being currently loaded by the page loader
    and puts the current URL back on the mPagesPending stack.

    </p>

    <h3>Overridable Methods</h3>

    <p>

    The following methods are intended to be customized in order
    to provide extended functionality to the spider. 

    </p>

    <h4>mOnStart</h4>

    <p>

    mOnStart() is called by run(). Return true to begin loading pages,
    false to re-enter the 'ready' state.

    </p>

    <h4>mOnBeforePage</h4>

    <p>

    mOnBeforePage() is called by loadPage(). Return true to continue loading
    the page, false to enter the 'paused' state. Note that when onBeforePage is
    called, mCurrentUrl will contain the CUrl object for the next page to be
    loaded.

    </p>

    <h4>mOnAfterPage</h4>

    <p>

    mOnAfterPage() is called by onLoadPage after all new links have been 
    added to mPagesPending. Return true to load the next page or false 
    to enter the 'paused' state. Note that when mOnAfterPage is called,
    mDocument will contain a reference to the currently loaded document.

    </p>

    <h4>mOnPageTimeout</h4>

    <p>

    mOnPageTimeout() will be called if the page does not complete loading
    in mOnLoadTimeoutInterval milliseconds. Return true to enter the 'timeout'
    state, false to attempt to load the next page.

    </p>

    <h4>mOnStop</h4>

    <p>

    mOnStop() is called by stop. Return true to enter the 'stopped' state,
    false to attempt to load the next page.

    </p>

    <h4>mOnPause</h4>

    <p>

    mOnPause() is called by pause. Return true to enter the 'paused' state,
    false to attempt to load the next page.

    </p>

    <h4>mOnRestart</h4>

    <p>

    mOnRestart() is called by restart. Return true to load the next page,
    false to re-enter the 'paused' state.

    </p>

  </body>
</html>

